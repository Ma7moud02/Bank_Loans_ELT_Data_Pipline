# Loan Default Prediction â€“ Big Data ETL Pipeline  

## ğŸ“Œ Overview  
This project demonstrates a **Big Data ETL Pipeline** for analyzing loan defaults.  

We start by setting up **Postgres inside Docker**, extract data with **Sqoop** into **HDFS**, perform transformations and modeling using **Spark in Zeppelin**, load the final tables into **Hive**, and finally visualize insights with **Power BI**.  

---

## ğŸ› ï¸ Tools & Technologies  
- **Docker** â†’ Containerized environment for all services  
- **PgAdmin** (`http://localhost:5000/`) â†’ Postgres database management  
- **Postgres** â†’ Source database (raw financial loan dataset)  
- **Sqoop** â†’ Data extraction from Postgres â†’ HDFS  
- **HDFS** â†’ Distributed storage for extracted data  
- **Spark (PySpark) in Zeppelin** (`http://localhost:8082/`) â†’ Data cleaning, transformation, and dimensional modeling  
- **Hive** â†’ Data warehouse for analytics  
- **Power BI** â†’ Data visualization and dashboards  

---

## ğŸ”„ ETL Pipeline Steps  

### 1. Setup & Data Loading â€“ Postgres  
- Opened **PgAdmin** on:  
