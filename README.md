# Loan Default Prediction – Big Data ETL Pipeline  

## 📌 Overview  
This project demonstrates a **Big Data ETL Pipeline** for analyzing loan defaults.  

We start by setting up **Postgres inside Docker**, extract data with **Sqoop** into **HDFS**, perform transformations and modeling using **Spark in Zeppelin**, load the final tables into **Hive**, and finally visualize insights with **Power BI**.  

---

## 🛠️ Tools & Technologies  
- **Docker** → Containerized environment for all services  
- **PgAdmin** (`http://localhost:5000/`) → Postgres database management  
- **Postgres** → Source database (raw financial loan dataset)  
- **Sqoop** → Data extraction from Postgres → HDFS  
- **HDFS** → Distributed storage for extracted data  
- **Spark (PySpark) in Zeppelin** (`http://localhost:8082/`) → Data cleaning, transformation, and dimensional modeling  
- **Hive** → Data warehouse for analytics  
- **Power BI** → Data visualization and dashboards  

---

## 🔄 ETL Pipeline Steps  

### 1. Setup & Data Loading – Postgres  
- Opened **PgAdmin** on:  
http://localhost:5000/
- Created a database and loaded the `financial_loan` table into Postgres.  

📸 *Screenshots to include:*  
- PgAdmin interface showing `financial_loan` table.  
- Terminal output confirming data is available in Postgres.  

---

### 2. Data Extraction – Sqoop  
- Opened Sqoop inside the Hive container:  
```bash
docker exec -it hive-server bash
