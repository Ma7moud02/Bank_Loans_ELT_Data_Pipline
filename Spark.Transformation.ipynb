{
  "metadata": {
    "name": "capstone1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import col, sum, avg, countDistinct, desc, count ,bround , from_unixtime\r\nfrom pyspark.sql.functions import col, udf\r\nfrom pyspark.sql.types import StringType\r\nfrom pyspark.sql.functions import regexp_replace, col\r\n\r\nspark \u003d SparkSession.builder.appName(\"ClickstreamETL\").enableHiveSupport().getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df \u003d spark.read.parquet(\"/staging_zone/financial_loan\") \n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df\u003dfinancial_df.withColumn(\"issue_date\",from_unixtime(col(\"issue_date\")/1000).cast(\"date\"))\nfinancial_df\u003dfinancial_df.withColumn(\"last_credit_pull_date\",from_unixtime(col(\"last_credit_pull_date\")/1000).cast(\"date\"))\nfinancial_df\u003dfinancial_df.withColumn(\"last_payment_date\",from_unixtime(col(\"last_payment_date\")/1000).cast(\"date\"))\nfinancial_df\u003dfinancial_df.withColumn(\"next_payment_date\",from_unixtime(col(\"next_payment_date\")/1000).cast(\"date\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Complete mapping dictionary (all US states + DC)\nstate_dict \u003d {\n    \"AL\": \"Alabama\",\n    \"AK\": \"Alaska\",\n    \"AZ\": \"Arizona\",\n    \"AR\": \"Arkansas\",\n    \"CA\": \"California\",\n    \"CO\": \"Colorado\",\n    \"CT\": \"Connecticut\",\n    \"DE\": \"Delaware\",\n    \"FL\": \"Florida\",\n    \"GA\": \"Georgia\",\n    \"HI\": \"Hawaii\",\n    \"ID\": \"Idaho\",\n    \"IL\": \"Illinois\",\n    \"IN\": \"Indiana\",\n    \"IA\": \"Iowa\",\n    \"KS\": \"Kansas\",\n    \"KY\": \"Kentucky\",\n    \"LA\": \"Louisiana\",\n    \"ME\": \"Maine\",\n    \"MD\": \"Maryland\",\n    \"MA\": \"Massachusetts\",\n    \"MI\": \"Michigan\",\n    \"MN\": \"Minnesota\",\n    \"MS\": \"Mississippi\",\n    \"MO\": \"Missouri\",\n    \"MT\": \"Montana\",\n    \"NE\": \"Nebraska\",\n    \"NV\": \"Nevada\",\n    \"NH\": \"New Hampshire\",\n    \"NJ\": \"New Jersey\",\n    \"NM\": \"New Mexico\",\n    \"NY\": \"New York\",\n    \"NC\": \"North Carolina\",\n    \"ND\": \"North Dakota\",\n    \"OH\": \"Ohio\",\n    \"OK\": \"Oklahoma\",\n    \"OR\": \"Oregon\",\n    \"PA\": \"Pennsylvania\",\n    \"RI\": \"Rhode Island\",\n    \"SC\": \"South Carolina\",\n    \"SD\": \"South Dakota\",\n    \"TN\": \"Tennessee\",\n    \"TX\": \"Texas\",\n    \"UT\": \"Utah\",\n    \"VT\": \"Vermont\",\n    \"VA\": \"Virginia\",\n    \"WA\": \"Washington\",\n    \"WV\": \"West Virginia\",\n    \"WI\": \"Wisconsin\",\n    \"WY\": \"Wyoming\",\n    \"DC\": \"District of Columbia\"\n}\n\n# Example DataFrame with state codes\n\n# Create a UDF to map codes to full names\ncode_to_name_udf \u003d udf(lambda code: state_dict.get(code, \"Unknown\"), StringType())\n\n# Add full state name column\nfinancial_df\u003d financial_df.withColumn(\"address_state\", code_to_name_udf(col(\"address_state\")))\n\nfinancial_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, regexp_replace, trim\n\nfinancial_df \u003d financial_df.withColumn(\n    \"emp_length\",\n    trim(regexp_replace(col(\"emp_length\"), \" years?|year\", \"\"))\n)\n\nfinancial_df.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df \u003d financial_df.withColumn(\n    \"term\",\n    trim(regexp_replace(col(\"term\"), \" months?|months\", \"\"))\n)\n\nfinancial_df.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df\u003d financial_df.fillna({\"emp_title\": \"Unknown\"})"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df \u003d financial_df.withColumn(\n    \"emp_length\",\n    trim(\n        regexp_replace(col(\"emp_length\"), \"[\u003c\u003e\\\\+]\", \"\")\n    )\n)\n\nfinancial_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col\n\nfinancial_df \u003d financial_df.withColumn(\n    \"emp_length\",\n    col(\"emp_length\").cast(\"int\")\n)\n\nfinancial_df.printSchema()\nfinancial_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfinancial_df.write \\\r\n    .mode(\"overwrite\") \\\r\n    .format(\"parquet\") \\\r\n    .saveAsTable(\"Financial_essential\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\ndim_borrowers \u003d financial_df.select(\n    col(\"member_id\").alias(\"borrowers_id_bk\"),   # Business Key\n    col(\"emp_title\").alias(\"employment_title\"),\n    col(\"emp_length\").alias(\"employment_length\"),\n    col(\"annual_income\"),\n    col(\"home_ownership\"),\n    col(\"address_state\").alias(\"state_code\"),\n    col(\"total_acc\").alias(\"total_account\"),\n    col(\"verification_status\"),\n    col(\"application_type\")\n).dropDuplicates([\"borrowers_id_bk\"]) \\\n .withColumn(\"borrowers_id_sk\", monotonically_increasing_id())\n\n# نخلي الـ surrogate key أول عمود\ndim_borrowers \u003d dim_borrowers.select(\n    \"borrowers_id_sk\",\n    \"borrowers_id_bk\",\n    \"employment_title\",\n    \"employment_length\",\n    \"annual_income\",\n    \"home_ownership\",\n    \"state_code\",\n    \"total_account\",\n    \"verification_status\",\n    \"application_type\"\n)\n\ndim_borrowers.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, monotonically_increasing_id, when\n\ndim_status \u003d financial_df.select(\n    col(\"loan_status\").alias(\"loan_status\")\n).dropDuplicates([\"loan_status\"]) \\\n .withColumn(\"status_id_sk\", monotonically_increasing_id())\n\n# Add loan_status_category column\ndim_status \u003d dim_status.withColumn(\n    \"loan_status_category\",\n    when(col(\"loan_status\").isin(\"Fully Paid\", \"Current\"), \"Good\")\n    .otherwise(\"Bad\")\n)\n\n# Final selection\ndim_status \u003d dim_status.select(\n    \"status_id_sk\",\n    col(\"loan_status\").alias(\"status_id\"),\n    \"loan_status_category\"\n)\n\ndim_status.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_credit_grade \u003d financial_df.select(\n    col(\"grade\"),\n    col(\"sub_grade\")\n).dropDuplicates([\"sub_grade\"]) \\\n .withColumn(\"credit_grade_sk\", monotonically_increasing_id())\n\ndim_credit_grade \u003d dim_credit_grade.select(\n    \"credit_grade_sk\",\n    \"grade\",\n    \"sub_grade\"\n)\ndim_credit_grade.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, monotonically_increasing_id, concat_ws, lit\n\n# Step 1: نجيب العمود period\ndim_loan_term \u003d financial_df.select(\n    col(\"term\").alias(\"period\")\n).dropDuplicates([\"period\"]) \\\n .withColumn(\"loan_term_sk\", monotonically_increasing_id())\n\n# Step 2: نضيف الوصف term_description\ndim_loan_term \u003d dim_loan_term.select(\n    \"loan_term_sk\",\n    \"period\"\n).withColumn(\n    \"term_description\",\n    concat_ws(\" \", col(\"period\"), lit(\"months\"))\n)\n\ndim_loan_term.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom datetime import datetime, timedelta\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, year, month, date_format\nfrom pyspark.sql.types import IntegerType\n\n# Generate all dates for 2021\nstart_date \u003d datetime(2021, 1, 1)\nend_date \u003d datetime(2021, 12, 31)\n\ndate_list \u003d [(start_date + timedelta(days\u003dx),) for x in range((end_date - start_date).days + 1)]\n\ndf_dates \u003d spark.createDataFrame(date_list, [\"Date\"])\n\n# Add Date_key in YYYYMMDD format\ndf_dates \u003d df_dates.withColumn(\"Date_key\", F.date_format(col(\"Date\"), \"yyyyMMdd\").cast(IntegerType()))\n\n# Extract Year, Month, Month_name, Quarter\ndf_dates \u003d df_dates.withColumn(\"Year\", year(col(\"Date\"))) \\\n                   .withColumn(\"Month\", month(col(\"Date\"))) \\\n                   .withColumn(\"Month_name\", date_format(col(\"Date\"), \"MMMM\")) \\\n                   .withColumn(\"Quarter\", F.quarter(col(\"Date\")))\n\n# Reorder columns\ndf_dates \u003d df_dates.select(\"Date_key\", \"Date\", \"Year\", \"Month\", \"Month_name\", \"Quarter\")\n\ndf_dates.show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, to_date, monotonically_increasing_id\n\n# الخطوة الأولى: تحويل أعمدة التاريخ وعمل الربط المبدئي مع جدول التواريخ\n# (هذه الجزئية من الكود الأصلي صحيحة، مع تعديل بسيط على الأسماء لتطابق الرسم)\nfinancial_df_with_dates \u003d financial_df \\\n    .withColumn(\"issue_date_dt\", to_date(col(\"issue_date\"), \"yyyy-MM-dd\")) \\\n    .withColumn(\"last_payment_date_dt\", to_date(col(\"last_payment_date\"), \"yyyy-MM-dd\")) \\\n    .withColumn(\"next_payment_date_dt\", to_date(col(\"next_payment_date\"), \"yyyy-MM-dd\")) \\\n    .withColumn(\"last_credit_pull_date_dt\", to_date(col(\"last_credit_pull_date\"), \"yyyy-MM-dd\"))\n\n# الربط مع جدول التواريخ للحصول على المفاتيح الخاصة بكل تاريخ\nfact_loan_wip \u003d financial_df_with_dates \\\n    .join(df_dates.alias(\"d_issue\"), col(\"issue_date_dt\") \u003d\u003d col(\"d_issue.Date\"), \"left\") \\\n    .join(df_dates.alias(\"d_last_pay\"), col(\"last_payment_date_dt\") \u003d\u003d col(\"d_last_pay.Date\"), \"left\") \\\n    .join(df_dates.alias(\"d_next_pay\"), col(\"next_payment_date_dt\") \u003d\u003d col(\"d_next_pay.Date\"), \"left\") \\\n    .join(df_dates.alias(\"d_credit_pull\"), col(\"last_credit_pull_date_dt\") \u003d\u003d col(\"d_credit_pull.Date\"), \"left\")\n\n# الخطوة الثانية: الربط مع باقي جداول الأبعاد للحصول على المفاتيح الاصطناعية (Surrogate Keys)\nfact_loan_wip \u003d fact_loan_wip \\\n    .join(dim_borrowers, financial_df_with_dates.member_id \u003d\u003d dim_borrowers.borrowers_id_bk, \"left\") \\\n    .join(dim_status, financial_df_with_dates.loan_status \u003d\u003d dim_status.status_id, \"left\") \\\n    .join(dim_credit_grade, financial_df_with_dates.sub_grade \u003d\u003d dim_credit_grade.sub_grade, \"left\") \\\n    .join(dim_loan_term, financial_df_with_dates.term \u003d\u003d dim_loan_term.period, \"left\")\n\n# الخطوة الثالثة: اختيار الأعمدة النهائية وإنشاء المفتاح الأساسي لجدول الحقائق\nfact_loan \u003d fact_loan_wip.select(\n    # Business Key\n    col(\"id\").alias(\"loan_id_bk\"),\n    \n    # Foreign Keys from Dimension Tables\n    col(\"borrowers_id_sk\").alias(\"borrowers_id_fk\"),\n    col(\"status_id_sk\").alias(\"status_id_fk\"),\n    col(\"credit_grade_sk\").alias(\"credit_grade_fk\"),\n    col(\"loan_term_sk\").alias(\"loan_term_fk\"),\n    col(\"d_issue.Date_key\").alias(\"date_key_issue\"),\n    col(\"d_last_pay.Date_key\").alias(\"date_key_last_payment\"),\n    col(\"d_next_pay.Date_key\").alias(\"date_key_next_payment\"),\n    col(\"d_credit_pull.Date_key\").alias(\"last_credit_pull_date\"),\n    \n    # Measures (المقاييس الرقمية)\n    col(\"loan_amount\"),\n    col(\"dti\").alias(\"DTI\"),\n    col(\"installment\"),\n    col(\"int_rate\").alias(\"interest_rate\"),\n    col(\"total_payment\"),\n    col(\"purpose\").alias(\"loan_purpose\")\n).withColumn(\"loan_id_pk_sk\", monotonically_increasing_id())\n\n# إعادة ترتيب الأعمدة لتطابق الرسم ووضع المفتاح الأساسي في البداية\nfact_loan \u003d fact_loan.select(\n    \"loan_id_pk_sk\",\n    \"loan_id_bk\",\n    \"borrowers_id_fk\",\n    \"status_id_fk\",\n    \"credit_grade_fk\",\n    \"loan_term_fk\",\n    \"date_key_issue\",\n    \"date_key_last_payment\",\n    \"date_key_next_payment\",\n    \"last_credit_pull_date\",\n    \"loan_amount\",\n    \"DTI\",\n    \"installment\",\n    \"interest_rate\",\n    \"total_payment\",\n    \"loan_purpose\"\n)\n\nfact_loan.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nspark.sql(\"USE default\")\r\ndim_borrowers.write \\\r\n    .mode(\"overwrite\") \\\r\n    .format(\"parquet\") \\\r\n    .saveAsTable(\"dim_borrowers\")\r\n\r\ndim_credit_grade.write \\\r\n    .mode(\"overwrite\") \\\r\n    .format(\"parquet\") \\\r\n    .saveAsTable(\"dim_credit_grade\")\r\n\r\ndim_status.write \\\r\n    .mode(\"overwrite\") \\\r\n    .format(\"parquet\") \\\r\n    .saveAsTable(\"dim_status\")\r\n\r\ndim_loan_term.write \\\r\n    .mode(\"overwrite\") \\\r\n    .format(\"parquet\") \\\r\n    .saveAsTable(\"dim_loan_term\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_dates.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"dim_date\")"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Save fact_loan to Hive\nfact_loan.write \\\n    .format(\"parquet\") \\\n    .mode(\"overwrite\") \\\n    .saveAsTable(\"fact_loan\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark"
    }
  ]
}