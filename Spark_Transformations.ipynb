{
  "metadata": {
    "name": "Bank_loans",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import col, regexp_replace, trim, udf, to_date, from_unixtime\r\nfrom pyspark.sql.types import StringType, LongType\r\n\r\n# Start Spark with Hive support\r\nspark \u003d SparkSession.builder \\\r\n    .appName(\"BankLoans\") \\\r\n    .enableHiveSupport() \\\r\n    .getOrCreate()\r\n\r\n# 1. Load raw parquet from staging-zone (Sqoop import output)\r\nfinancial_df \u003d spark.read.parquet(\"/staging_zone_6/financial_loan\")\r\n\r\n# 2. Handle date columns robustly\r\ndate_cols \u003d [\"issue_date\", \"last_credit_pull_date\", \"last_payment_date\", \"next_payment_date\"]\r\nfor c in date_cols:\r\n    dtype \u003d dict(financial_df.dtypes)[c]\r\n    if dtype in [\"bigint\", \"long\", \"int\"]:\r\n        # UNIX milliseconds → divide by 1000 → to_date\r\n        financial_df \u003d financial_df.withColumn(c, to_date(from_unixtime(col(c).cast(\"double\") / 1000)))\r\n    else:\r\n        # Assume string in dd-MM-yyyy\r\n        financial_df \u003d financial_df.withColumn(c, to_date(col(c), \"dd-MM-yyyy\"))\r\n\r\n# 3. Map state codes → full names\r\nstate_dict \u003d {\r\n    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\r\n    \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\",\r\n    \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\",\r\n    \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\", \"MA\": \"Massachusetts\",\r\n    \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\", \"MT\": \"Montana\",\r\n    \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\",\r\n    \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\r\n    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\r\n    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\",\r\n    \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\",\r\n    \"WY\": \"Wyoming\", \"DC\": \"District of Columbia\"\r\n}\r\ncode_to_name_udf \u003d udf(lambda code: state_dict.get(code, \"Unknown\"), StringType())\r\nfinancial_df \u003d financial_df.withColumn(\"address_state\", code_to_name_udf(col(\"address_state\")))\r\n\r\n# 4. Clean emp_length column\r\nfinancial_df \u003d financial_df.withColumn(\"emp_length\", trim(regexp_replace(col(\"emp_length\"), \"[\u003c\u003e\\\\+]| years?|year\", \"\")))\r\n\r\n# 5. Clean term column\r\nfinancial_df \u003d financial_df.withColumn(\"term\", trim(regexp_replace(col(\"term\"), \" months?\", \"\")))\r\n\r\n# 6. Fill nulls for categorical fields\r\nfinancial_df \u003d financial_df.fillna({\r\n    \"emp_title\": \"Unknown\",\r\n    \"application_type\": \"Unknown\",\r\n    \"loan_status\": \"Unknown\"\r\n})\r\n\r\n# Show sample\r\nfinancial_df.show(10, truncate\u003dFalse)\r\n\r\n# 7. Save cleaned dataset into clean-zone\r\nfinancial_df.write.mode(\"overwrite\").parquet(\"/clean-zone/financial_loan_cleaned\")\r\n\r\nprint(\"✅ Step complete: Cleaned data written to /clean-zone/financial_loan_cleaned\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark \u003d SparkSession.builder \\\n    .appName(\"BankLoans\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"SHOW DATABASES\").show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinancial_df.write.mode(\"overwrite\") \\\n    .format(\"hive\") \\\n    .saveAsTable(\"default.financial_loan_cleaned\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\ndim_borrowers \u003d financial_df.select(\n    col(\"member_id\").alias(\"borrowers_id_bk\"),   # Business Key\n    col(\"emp_title\").alias(\"employment_title\"),\n    col(\"emp_length\").alias(\"employment_length\"),\n    col(\"annual_income\"),\n    col(\"home_ownership\"),\n    col(\"address_state\").alias(\"state_code\"),\n    col(\"total_acc\").alias(\"total_account\"),\n    col(\"verification_status\"),\n    col(\"application_type\")\n).dropDuplicates([\"borrowers_id_bk\"]) \\\n .withColumn(\"borrowers_id_sk\", monotonically_increasing_id())\n\n# نخلي الـ surrogate key أول عمود\ndim_borrowers \u003d dim_borrowers.select(\n    \"borrowers_id_sk\",\n    \"borrowers_id_bk\",\n    \"employment_title\",\n    \"employment_length\",\n    \"annual_income\",\n    \"home_ownership\",\n    \"state_code\",\n    \"total_account\",\n    \"verification_status\",\n    \"application_type\"\n)\n\ndim_borrowers.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_borrowers.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"dim_borrowers\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, monotonically_increasing_id, when\n\ndim_status \u003d financial_df.select(\n    col(\"loan_status\").alias(\"loan_status\")\n).dropDuplicates([\"loan_status\"]) \\\n .withColumn(\"status_id_sk\", monotonically_increasing_id())\n\n# Add loan_status_category column\ndim_status \u003d dim_status.withColumn(\n    \"loan_status_category\",\n    when(col(\"loan_status\").isin(\"Fully Paid\", \"Current\"), \"Good\")\n    .otherwise(\"Bad\")\n)\n\n# Final selection\ndim_status \u003d dim_status.select(\n    \"status_id_sk\",\n    col(\"loan_status\").alias(\"status_id\"),\n    \"loan_status_category\"\n)\n\ndim_status.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_status.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"dim_status\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_credit_grade \u003d financial_df.select(\n    col(\"grade\"),\n    col(\"sub_grade\")\n).dropDuplicates([\"sub_grade\"]) \\\n .withColumn(\"credit_grade_sk\", monotonically_increasing_id())\n\ndim_credit_grade \u003d dim_credit_grade.select(\n    \"credit_grade_sk\",\n    \"grade\",\n    \"sub_grade\"\n)\ndim_credit_grade.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_credit_grade.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"dim_credit_grade\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, monotonically_increasing_id, concat_ws, lit\n\n# Step 1: نجيب العمود period\ndim_loan_term \u003d financial_df.select(\n    col(\"term\").alias(\"period\")\n).dropDuplicates([\"period\"]) \\\n .withColumn(\"loan_term_sk\", monotonically_increasing_id())\n\n# Step 2: نضيف الوصف term_description\ndim_loan_term \u003d dim_loan_term.select(\n    \"loan_term_sk\",\n    \"period\"\n).withColumn(\n    \"term_description\",\n    concat_ws(\" \", col(\"period\"), lit(\"months\"))\n)\n\ndim_loan_term.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_loan_term.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"dim_loan_term\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, year, month, dayofmonth, date_format, lit\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import functions as F\nfrom datetime import datetime, timedelta\n\nspark \u003d SparkSession.builder \\\n    .appName(\"StarSchema\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Generate all dates for 2021\nstart_date \u003d datetime(2021, 1, 1)\nend_date \u003d datetime(2021, 12, 31)\n\ndate_list \u003d [(start_date + timedelta(days\u003dx),) for x in range((end_date - start_date).days + 1)]\n\ndf_dates \u003d spark.createDataFrame(date_list, [\"Date\"])\n\n# Add Date_key in YYYYMMDD format\ndf_dates \u003d df_dates.withColumn(\"Date_key\", F.date_format(col(\"Date\"), \"yyyyMMdd\").cast(IntegerType()))\n\n# Extract Year, Month, Month_name, Quarter\ndf_dates \u003d df_dates.withColumn(\"Year\", year(col(\"Date\"))) \\\n                   .withColumn(\"Month\", month(col(\"Date\"))) \\\n                   .withColumn(\"Month_name\", date_format(col(\"Date\"), \"MMMM\")) \\\n                   .withColumn(\"Quarter\", F.quarter(col(\"Date\")))\n\n# Reorder columns\ndf_dates \u003d df_dates.select(\"Date_key\", \"Date\", \"Year\", \"Month\", \"Month_name\", \"Quarter\")\n\ndf_dates.show(5, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Save the dim_date table to Hive\ndf_dates.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"dim_date\")  # this creates the table in Hive\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.sql.functions import col, to_date, monotonically_increasing_id\r\n\r\n# Convert string columns to date\r\nfinancial_df \u003d financial_df \\\r\n    .withColumn(\"issue_date_dt\", to_date(col(\"issue_date\"), \"yyyy-MM-dd\")) \\\r\n    .withColumn(\"last_payment_date_dt\", to_date(col(\"last_payment_date\"), \"yyyy-MM-dd\")) \\\r\n    .withColumn(\"next_payment_date_dt\", to_date(col(\"next_payment_date\"), \"yyyy-MM-dd\")) \\\r\n    .withColumn(\"last_credit_pull_date_dt\", to_date(col(\"last_credit_pull_date\"), \"yyyy-MM-dd\"))\r\n\r\n# Join with dim_date for issue_date\r\nfact_loan \u003d financial_df.join(\r\n    df_dates.select(col(\"Date_key\").alias(\"date_key_issue\"), col(\"Date\").alias(\"issue_date_dim\")),\r\n    financial_df.issue_date_dt \u003d\u003d col(\"issue_date_dim\"),\r\n    \"left\"\r\n)\r\n\r\n# Join with dim_date for last_payment_date\r\nfact_loan \u003d fact_loan.join(\r\n    df_dates.select(col(\"Date_key\").alias(\"date_key_last_payment\"), col(\"Date\").alias(\"last_payment_date_dim\")),\r\n    fact_loan.last_payment_date_dt \u003d\u003d col(\"last_payment_date_dim\"),\r\n    \"left\"\r\n)\r\n\r\n# Join with dim_date for next_payment_date\r\nfact_loan \u003d fact_loan.join(\r\n    df_dates.select(col(\"Date_key\").alias(\"date_key_next_payment\"), col(\"Date\").alias(\"next_payment_date_dim\")),\r\n    fact_loan.next_payment_date_dt \u003d\u003d col(\"next_payment_date_dim\"),\r\n    \"left\"\r\n)\r\n\r\n# Join with dim_date for last_credit_pull_date\r\nfact_loan \u003d fact_loan.join(\r\n    df_dates.select(col(\"Date_key\").alias(\"date_key_last_credit_pull\"), col(\"Date\").alias(\"last_credit_pull_date_dim\")),\r\n    fact_loan.last_credit_pull_date_dt \u003d\u003d col(\"last_credit_pull_date_dim\"),\r\n    \"left\"\r\n)\r\n\r\n# Select final columns\r\nfact_loan \u003d fact_loan.select(\r\n    col(\"id\").alias(\"loan_id_bk\"),\r\n    col(\"member_id\").alias(\"borrowers_id_k\"),\r\n    col(\"date_key_issue\"),\r\n    col(\"date_key_last_payment\"),\r\n    col(\"date_key_next_payment\"),\r\n    col(\"date_key_last_credit_pull\"),\r\n    col(\"grade\").alias(\"credit_grade_k\"),\r\n    col(\"term\").alias(\"loan_term_lk\"),\r\n    col(\"loan_amount\"),\r\n    col(\"dti\").alias(\"DTI\"),\r\n    col(\"installment\"),\r\n    col(\"int_rate\").alias(\"interest_rate\"),\r\n    col(\"total_payment\"),\r\n    col(\"purpose\").alias(\"loan_purpose\"),\r\n    col(\"loan_status\").alias(\"status_id_k\")\r\n).withColumn(\"loan_id_pk_sk\", monotonically_increasing_id())\r\n\r\nfact_loan.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n# Save fact_loan to Hive\r\nfact_loan.write \\\r\n    .format(\"parquet\") \\\r\n    .mode(\"overwrite\") \\\r\n    .saveAsTable(\"fact_loan\")\r\n"
    }
  ]
}